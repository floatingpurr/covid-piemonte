name: piemonte-data-scraper

# Schedule the run
on:
  workflow_dispatch:
  schedule:
    - cron:  '*/30 * * * *'

jobs:
  # Set the job key.
  scrape-data:
    # Name of the Job
    name: Scrape data
    # Set the type of machine to run on
    runs-on: ubuntu-latest

    steps:
      # Check out a copy of this repository on the ubuntu-latest machine
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Install dependencies from requirements.txt
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Get data
      - name: Get data
        run: python src/scraper.py
        env:
          PROXY_HOST: ${{ secrets.PROXY_HOST }}
          API_USER: ${{ secrets.API_USER }}
          API_PASSWORD: ${{ secrets.API_PASSWORD }}
          LOGIN_PATH: ${{ secrets.LOGIN_PATH }}
          API_PATH: ${{ secrets.API_PATH }}

      # Commit & Push
      - name: Commit a new report (if necessary)
        run: |
          echo "Checking data on: `date`"
          if cmp -s data/staging-covid-piemonte.csv data/covid-piemonte.csv ; then
            echo "No changes to commit";
          else
            echo "New update available";
            cp data/{metadata.txt,covid-piemonte.csv} data/previous/
            mv data/staging-covid-piemonte.csv data/covid-piemonte.csv
            mv data/staging-metadata.txt data/metadata.txt
            git config --global user.name 'GitHub Actions'
            git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
            git commit -am "Refresh data on: `date`"
            git push
          fi
        env:
          TZ: Europe/Rome
